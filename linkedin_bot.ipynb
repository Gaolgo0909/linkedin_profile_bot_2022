{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the libraries that will be used in the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tools used to WebSrapping\n",
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "# Tools used to data analysis\n",
    "import time\n",
    "import pandas as pd\n",
    "import json\n",
    "import math\n",
    "import itertools\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. This notebook section is responsible for collecting the URLS and the number of employees of each of the selected companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_list=list(pd.read_excel(\"companiy_list.xlsx\")[\"companies\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1 Using a bot to login into Linkedin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver  = webdriver.Chrome(ChromeDriverManager().install())\n",
    "driver.set_window_size(1920, 1800)\n",
    "driver.maximize_window()\n",
    "email= \"email\" \n",
    "password = \"password\"\n",
    "\n",
    "driver.get(\"https://www.linkedin.com/login\")\n",
    "time.sleep(2)\n",
    "\n",
    "eml = driver.find_element(by=By.ID, value=\"username\")\n",
    "eml.send_keys(email)\n",
    "passwd = driver.find_element(by=By.ID, value=\"password\")\n",
    "passwd.send_keys(password)\n",
    "loginbutton = driver.find_element(by=By.XPATH, value=\"//*[@id=\\\"organic-div\\\"]/form/div[3]/button\")\n",
    "loginbutton.click()\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2 Searching for the companies profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell finds the url's of most companies, but some are not found, because Linkedin complicates the collection of information from profiles, so the next cell works to find the ones that were not found in this first one.\n",
    "companies_dict = {}\n",
    "not_found_list1=[]\n",
    "def finding_jobs():\n",
    "        for i in companies_list:\n",
    "            try:\n",
    "                driver.get(\"https://www.linkedin.com/search/results/companies/?keywords={}&sid=LeK\".format(i))\n",
    "                time.sleep(3)\n",
    "                companies_dict[i]=driver.find_element(By.XPATH, '/html/body/div[5]/div[3]/div[2]/div/div[1]/main/div/div/div[1]/ul/li[1]/div/div/div[2]/div[1]/div[1]/div/span/span/a' ).get_attribute(\"href\")\n",
    "                time.sleep(3)\n",
    "            except:\n",
    "                not_found_list1.append(i)\n",
    "                pass\n",
    "finding_jobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell ends the job of finding the url's of the companies. It's important to notice that not all companies will be found. That can happen becaue the companie name is not the same as the name in Linkedin, or because the company is not in Linkedin.\n",
    "wrong_names_list=[]\n",
    "def finding_jobs2():\n",
    "    for i in not_found_list1:\n",
    "        try:\n",
    "            driver.get(\"https://www.linkedin.com/search/results/companies/?keywords={}&sid=LeK\".format(i))\n",
    "            time.sleep(2)\n",
    "            companies_dict[i]=driver.find_element(By.XPATH, '/html/body/div[4]/div[3]/div[2]/div/div[1]/main/div/div/div[1]/ul/li[1]/div/div/div[2]/div[1]/div[1]/div/span/span/a' ).get_attribute(\"href\")\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            wrong_names_list.append(i)\n",
    "            pass\n",
    "finding_jobs2() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell will convert the dictionary into a dataframe, to facilitate the analysis of the data.\n",
    "companies_url=pd.DataFrame(companies_dict.items(), columns=[\"companies\", \"url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Once we got the URLS of the companies, we can start to collect the data from their profiles. This cell is used to collect the numbers of employees of the company.\n",
    "companies_url_dict={}\n",
    "companies_name_dict={}\n",
    "dict_not_found={}\n",
    "list_erro=[]\n",
    "def getting_profiles():\n",
    "    for i in range(len(companies_url)):\n",
    "        driver.get(companies_url[\"url\"][i]+\"people/\")\n",
    "        time.sleep( random.uniform(2, 3))\n",
    "        driver.execute_script(\"window.scrollTo(0, 100);\")\n",
    "        try:\n",
    "            source = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            trab=\"\"\n",
    "            info = source.find('div', class_=\"display-flex full-width justify-space-between align-items-center pt5 ph5\")\n",
    "            for m in info.find(\"h2\",class_=\"t-20 t-black t-bold\").get_text().strip():\n",
    "                if m.isdigit():\n",
    "                    trab+=m\n",
    "            companies_url_dict[companies_url[\"url\"][i]]=companies_name_dict[companies_url[\"companies\"][i]]=trab\n",
    "        except:\n",
    "            list_erro.append(companies_url[\"companies\"][i])\n",
    "            pass\n",
    "getting_profiles()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the final DataFrame with the companie name, the url and the number of employees in the company.\n",
    "df=companies_url.merge(pd.DataFrame(companies_url_dict.items(), columns=[\"url\", \"number_of_employees\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting the number of employees to a numeric value. And sorting values\n",
    "def convert (x):\n",
    "    if x.isdigit():\n",
    "        return int(x)\n",
    "    else:\n",
    "        return 0\n",
    "df[\"number_of_employees\"]=df[\"number_of_employees\"].apply(convert)\n",
    "df=df.sort_values(by=\"number_of_employees\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. This notebook section will collect the URL for the employees of each companie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1 The next cell will gather all the URLS from each employee of each company"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.tail(40)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[]\n",
    "companie=[]\n",
    "for i,m, w in zip(df[\"url\"],df[\"number_of_employees\"],df[\"companies\"]):\n",
    "    driver.get( i +\"/people\")\n",
    "    time.sleep(random.uniform(2, 3))\n",
    "    # Linkedin doesn't automatically show all profiles, so we need to scroll up and down the page to make the data appear. \n",
    "    # For each company, we need to scroll up and down the page the number of times that is equal to the number of employees divided by 12, because Linkedin shows 12 profiles per page.\n",
    "    # The number of employees divided by 12 is rounded up, because we need to make sure that we will get all the profiles.\n",
    "    if int(m)<1020:\n",
    "        c=0\n",
    "        while c<int(m)/12+1:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(random.uniform(0.5, 2))\n",
    "            driver.execute_script(\"window.scrollTo(0, -1);\")\n",
    "            time.sleep(random.uniform(0.5, 2))\n",
    "            c+=1\n",
    "    else:\n",
    "        c=0\n",
    "        # Linkedin shows a maximum of something like 1000 profiles, so we need to make sure that we will not scroll a lot more than 1000 times. Seems that 1030 is the maximum number of scorlls that we may need to do.\n",
    "        while c<1030/12+1:\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(random.uniform(0.5, 2))\n",
    "            driver.execute_script(\"window.scrollTo(0, -1);\")\n",
    "            time.sleep(random.uniform(0.5, 2))\n",
    "            c+=1\n",
    "    #Finally, we get the url of each profile and add it to a list. Is important to get company name and url, because we will use it later to create a DataFrame.\n",
    "    for j in range(1,int(m)+1):\n",
    "        try:\n",
    "            urls.append(driver.find_element(By.XPATH, \"/html/body/div[5]/div[3]/div/div[2]/div/div[2]/main/div[2]/div/div[2]/div/div[1]/ul/li[{}]/div/section/div/div/div[2]/div[1]/a\".format(str(j))).get_attribute(\"href\"))\n",
    "            companie.append(w)\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. In this final section we will gather data from each profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1 The three following fucntions ( **Type2Jobs**, **ParseType1Job**, **returnProfileInfo** ) were found at: https://github.com/thelazyaz/linkedin-web-scraping, under no specific license. I add somechanges to fit my specific problem, but most of the code is the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get information about the profiles. It will get the data from jobs that were done in group. The information that will be gathered is: company, duration and the jobgroup.\n",
    "def Type2Jobs(alltext):\n",
    "    jobgroups = []\n",
    "    company = alltext[16][:len(alltext[16]) // 2]\n",
    "    totalDurationAtCompany = alltext[20][:len(alltext[20]) // 2]\n",
    "\n",
    "    groups = []\n",
    "    count = 0\n",
    "    index = 0\n",
    "    for a in alltext:\n",
    "        if a == '' or a == ' ':\n",
    "            count += 1\n",
    "        else:\n",
    "            groups.append((count, index))\n",
    "            count = 0\n",
    "        index += 1\n",
    "\n",
    "    numJobsInJoblist = [g for g in groups if g[0] == 21 or g[0] == 22 or g[0] == 25 or g[0] == 26]\n",
    "    for i in numJobsInJoblist:\n",
    "        if 'time' in alltext[i[1] + 5][:len(alltext[i[1] + 5]) // 2].lower().split('-'):\n",
    "            jobgroups.append((alltext[i[1]][:len(alltext[i[1]]) // 2], alltext[i[1] + 8][:len(alltext[i[1] + 8]) // 2]))\n",
    "        else:\n",
    "            jobgroups.append((alltext[i[1]][:len(alltext[i[1]]) // 2], alltext[i[1] + 4][:len(alltext[i[1] + 4]) // 2]))\n",
    "    return ('type2job', company, totalDurationAtCompany, jobgroups)\n",
    "\n",
    "# Function to get information about the profiles. It will get the data from individual jobs.  The information that will be gathered is the Job Title, Company, Duration, Location\n",
    "def parseType1Job(alltext):\n",
    "    jobtitle = alltext[16][:len(alltext[16]) // 2]\n",
    "    company = alltext[20][:len(alltext[20]) // 2]\n",
    "    duration = alltext[23][:len(alltext[23]) // 2]\n",
    "    location = alltext[26][:len(alltext[26]) // 2]\n",
    "    return ('type1job', jobtitle, company, duration, location)\n",
    "\n",
    "#Function to get the information from the profiles. It concate data that were gathered from the last two functions. \n",
    "# Additionally, it will get the name of the profile, experience, education, skills and certifications of each profile.\n",
    "def returnProfileInfo(employeeLink, companyName):\n",
    "    try:\n",
    "        url = employeeLink\n",
    "        driver.get(url)\n",
    "        time.sleep(random.uniform(2, 3))\n",
    "        source = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        profile = []\n",
    "        profile.append(companyName)\n",
    "        info = source.find('div', class_='mt2 relative')\n",
    "        name = info.find('h1', class_='text-heading-xlarge inline t-24 v-align-middle break-words').get_text().strip()\n",
    "        title = info.find('div', class_='text-body-medium break-words').get_text().lstrip().strip()\n",
    "        location = info.find('span', class_='text-body-small inline t-black--light break-words').get_text().strip()\n",
    "        profile.append(name)\n",
    "        profile.append(title)\n",
    "        profile.append(location)\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "        experiences = source.find_all('li', class_='artdeco-list__item pvs-list__item--line-separated pvs-list__item--one-column')\n",
    "\n",
    "        #Getting information about the experiences\n",
    "        for x in experiences[1:]:\n",
    "            alltext = x.getText().split('\\n')\n",
    "            print(alltext)\n",
    "            startIdentifier = 0\n",
    "            for e in alltext:\n",
    "                if e == '' or e == ' ':\n",
    "                    startIdentifier+=1\n",
    "                else:\n",
    "                    break\n",
    "            if startIdentifier == 16:\n",
    "                # Getting information about the educaticon\n",
    "                if 'universidade' in alltext[16].lower().split(' ') or 'faculdade' in alltext[16].lower().split(' ') or 'ba' in alltext[16].lower().split(' ') or 'bs' in alltext[16].lower().split(' '):\n",
    "                    profile.append(('education', alltext[16][:len(alltext[16])//2], alltext[20][:len(alltext[20])//2]))\n",
    "\n",
    "                # CGetting information about the certifications\n",
    "                elif 'Competências' in alltext[23].lower().split(' '):\n",
    "                    profile.append(('certification', alltext[16][:len(alltext[16])//2], alltext[20][:len(alltext[20])//2]))\n",
    "\n",
    "            elif startIdentifier == 12:\n",
    "                # Getting information about the skills\n",
    "                if (alltext[16] == '' or alltext[16] == ' ') and len(alltext) > 24:\n",
    "                    profile.append(('skill', alltext[12][:len(alltext[12])//2]))\n",
    "\n",
    "        # Getting information about the Jobs\n",
    "        url = driver.current_url + '/details/experience/'\n",
    "        driver.get(url)\n",
    "        time.sleep(random.uniform(2, 3))\n",
    "        source = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        time.sleep(random.uniform(1, 2))\n",
    "        exp = source.find_all('li')\n",
    "        for e in exp[13:]:\n",
    "            row = e.getText().split('\\n')\n",
    "            if row[:16] == ['', '', '', '', '', '', ' ', '', '', '', '', '', '', '', '', '']:\n",
    "                if 'yrs' in row[20].split(' '):\n",
    "                    profile.append(parseType2Jobs(row))\n",
    "                else:\n",
    "                    profile.append(parseType1Job(row))\n",
    "        return profile\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Using the functions to gather data about each of the profiles that we collected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prof=[]\n",
    "for url, emp in zip(urls, companie):\n",
    "    prof.append(returnProfileInfo(url, emp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Converting each profile to a row of a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "info=[]\n",
    "for i in range(len(prof)):\n",
    "    try:\n",
    "        skill=[]\n",
    "        job=[]\n",
    "        char=[]\n",
    "        for i in prof[i]:\n",
    "            if type(i)==tuple:\n",
    "                if \"skill\" in i:\n",
    "                    skill.append(i[1])\n",
    "                elif \"type1job\" in i:\n",
    "                    job.append(i[1:])\n",
    "            else:\n",
    "                char.append(i)\n",
    "        # We will limit the number of jobs to the first 5 that each profile has.\n",
    "        if len(job)<5: \n",
    "            while len(job)!=5:\n",
    "                job.append(\"\")\n",
    "        else:\n",
    "            job=job[:5]\n",
    "\n",
    "        # We will limit the number of skills to the first 5 that each profile has.\n",
    "        if len (skill)<5:\n",
    "            while len(skill)!=5:\n",
    "                skill.append(\"\")\n",
    "        else:\n",
    "            skill=skill[:5]\n",
    "        info.append(char+skill+job)\n",
    "    except:\n",
    "        pass\n",
    "    colunas=[\"Companie\",\"Name\",\"Job Title\",\"Location\",\"Skill1\",\"Skill2\",\"Skill3\",\"Skill4\",\"Skill5\",\"Current Job\",\"Past Job1\",\"Past Job2\",\"Past Job3\",\"Past Job4\"]\n",
    "#  Converting the data into a dataframe\n",
    "base= pd.DataFrame(info,columns=colunas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae00af8f4f373055312fd1fb604f892e76108128b66da296359186fbeac24a22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
